{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CourantEnCourant/rag_course/blob/main/dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "t_F-DVB9tFsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Charger un dataset depuis le Hub Hugging Face**"
      ],
      "metadata": {
        "id": "sQen-zds_zxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "# Charger un dataset complet\n",
        "squad_dataset = load_dataset(\"squad\")\n",
        "\n",
        "# Charger une configuration spécifique d'un dataset\n",
        "dataset = load_dataset(\"squad_v2\")\n",
        "\n",
        "# Charger uniquement certains splits (train, validation, test)\n",
        "train_dataset = load_dataset(\"squad\", split=\"train\")\n",
        "validation_dataset = load_dataset(\"squad\", split=\"validation\")\n",
        "\n",
        "# Charger un dataset hébergé par un utilisateur spécifique\n",
        "dataset = load_dataset(\"stanfordnlp/imdb\")"
      ],
      "metadata": {
        "id": "TJFBzEn-s7n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Charger des datasets depuis des fichiers locaux**"
      ],
      "metadata": {
        "id": "R_16b9QZ_71r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger depuis un fichier CSV\n",
        "dataset = load_dataset(\"csv\", data_files=\"mon_fichier.csv\")\n",
        "\n",
        "# Charger plusieurs fichiers en un seul dataset\n",
        "dataset = load_dataset(\"csv\", data_files=[\"fichier1.csv\", \"fichier2.csv\"])\n",
        "\n",
        "# Spécifier des fichiers différents pour différents splits\n",
        "dataset = load_dataset(\"csv\", data_files={\"train\": \"train.csv\", \"test\": \"test.csv\"})\n",
        "\n",
        "# Autres formats supportés\n",
        "json_dataset = load_dataset(\"json\", data_files=\"corpus.json\", field='data')\n",
        "text_dataset = load_dataset(\"text\", data_files=\"corpus.txt\")\n",
        "parquet_dataset = load_dataset(\"parquet\", data_files=\"donnees.parquet\")"
      ],
      "metadata": {
        "id": "sLxR-hvN9kfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher les informations générales\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "KriFlfmr-xS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explorer la structure des datasets**"
      ],
      "metadata": {
        "id": "gewqju2PAFyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Voir les features (colonnes) du dataset\n",
        "print(dataset[\"train\"].features)\n",
        "\n",
        "# A VOUS: Afficher quelques exemples\n",
        "\n",
        "# A VOUS: Accéder à une colonne spécifique: quelle est la question 200 du dataset? Et la réponse?\n",
        "\n",
        "\n",
        "# Obtenir des statistiques sur la taille du dataset\n",
        "print(f\"Nombre d'exemples dans l'ensemble d'entraînement : {len(dataset['train'])}\")\n",
        "print(f\"Nombre d'exemples dans l'ensemble de validation : {len(dataset['validation'])}\")"
      ],
      "metadata": {
        "id": "NkA5WYtH9mWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manipulation des datasets**\n",
        "\n",
        "La bibliothèque datasets offre de nombreuses fonctions pour manipuler vos datasets.\n",
        "\n",
        "**Filtrage et sélection**"
      ],
      "metadata": {
        "id": "m3I88TAgAtNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrer le dataset selon une condition\n",
        "\n",
        "long_questions = dataset[\"train\"].filter(lambda example: len(example[\"question\"]) > 20)\n",
        "print(f\"Nombre de questions longues : {len(long_questions)}\")\n"
      ],
      "metadata": {
        "id": "2QGtFU4uBFkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sélectionner un sous-ensemble aléatoire\n",
        "sample = dataset[\"train\"].shuffle(seed=42).select(range(1000))"
      ],
      "metadata": {
        "id": "qJSSq_LUuTuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrer avec plusieurs conditions\n",
        "filtered = dataset[\"train\"].filter(\n",
        "    lambda example: len(example[\"question\"]) > 10 and \"why\" in example[\"question\"].lower()\n",
        ")"
      ],
      "metadata": {
        "id": "l6ALailouW0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mapping et transformation**"
      ],
      "metadata": {
        "id": "UyFv_kTbBoBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Appliquer une fonction à chaque exemple\n",
        "def ajouter_longueur(example):\n",
        "    example[\"question_length\"] = len(example[\"question\"])\n",
        "    return example\n",
        "\n",
        "dataset_with_lengths = dataset[\"train\"].map(ajouter_longueur)"
      ],
      "metadata": {
        "id": "YVjTD0Xr_OoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Appliquer des transformations sur plusieurs processus pour accélérer\n",
        "transformed_dataset = dataset[\"train\"].map(\n",
        "    ajouter_longueur,\n",
        "    num_proc=4,  # Utiliser 4 processus\n",
        "    desc=\"Ajout des longueurs de questions\"  # Description pour la barre de progression\n",
        ")"
      ],
      "metadata": {
        "id": "LSew_QlXvEEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tri et regroupement**"
      ],
      "metadata": {
        "id": "l63mMBETB8cU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trier le dataset\n",
        "sorted_dataset = transformed_dataset.sort(\"question_length\")"
      ],
      "metadata": {
        "id": "uUriJ2OlIooN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouper par le premier mot:\n",
        "\n",
        "# A VOUS: Ajoutez la colonne \"first_word\" et afficher les 10 premiers mots les plus fréquents dans les questions\n",
        "\n"
      ],
      "metadata": {
        "id": "BnWZFrWPB_Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Traitement et transformation**"
      ],
      "metadata": {
        "id": "h07K4ZR5B_4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# A VOUS: tokénisez le dataset"
      ],
      "metadata": {
        "id": "0StNmf8ZCAdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Définir le format pour PyTorch, TensorFlow ou JAX\n",
        "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"])"
      ],
      "metadata": {
        "id": "P4AJa1OHwrI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualisez le premier exemple tokenizé:\n",
        "\n"
      ],
      "metadata": {
        "id": "QNGkDnzjK2KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sauvegarde et partage**"
      ],
      "metadata": {
        "id": "By5jGi7CDfd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sauvegarder un dataset sur disque\n",
        "dataset[\"train\"].save_to_disk(\"./mon_dataset_train\")\n",
        "\n",
        "# Charger depuis le disque\n",
        "from datasets import load_from_disk\n",
        "loaded_dataset = load_from_disk(\"./mon_dataset_train\")"
      ],
      "metadata": {
        "id": "9-CNY1yPEO4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Partager sur le Hub Hugging Face**"
      ],
      "metadata": {
        "id": "_HV61sYWEUJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pousser votre dataset sur le Hub\n",
        "dataset.push_to_hub(\"votre-username/nom-du-dataset\")\n"
      ],
      "metadata": {
        "id": "FDkHeH9kEUmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Création d'un DataLoader pour l'entraînement**"
      ],
      "metadata": {
        "id": "h8N7vKMYEi0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Convertir le dataset en format PyTorch\n",
        "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"])\n",
        "\n",
        "# Créer un DataLoader\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Utilisation dans une boucle d'entraînement\n",
        "for batch in train_dataloader:\n",
        "    # Votre code d'entraînement ici\n",
        "    print(batch[\"input_ids\"].shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "1Ecb0k12E37N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Créer votre propre dataset**"
      ],
      "metadata": {
        "id": "pLp4OJN0FBqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, Features, Value, ClassLabel\n",
        "\n",
        "# Données d'exemple\n",
        "data = {\n",
        "    \"text\": [\"J'adore ce film\", \"Ce film était terrible\", \"Film moyen\"],\n",
        "    \"label\": [1, 0, 1]\n",
        "}\n",
        "\n",
        "# Définir les features\n",
        "features = Features({\n",
        "    \"text\": Value(\"string\"),\n",
        "    \"label\": ClassLabel(names=[\"négatif\", \"positif\"])\n",
        "})\n",
        "\n",
        "# Créer le dataset\n",
        "my_dataset = Dataset.from_dict(data, features=features)\n",
        "print(my_dataset)"
      ],
      "metadata": {
        "id": "lBqbAG-EFaCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Streaming pour les grands datasets**"
      ],
      "metadata": {
        "id": "nRGU4lvFFeJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger un dataset en mode streaming\n",
        "streaming_dataset = load_dataset(\"c4\", \"en\", streaming=True)\n",
        "\n",
        "# Itérer sur le dataset sans le charger entièrement en mémoire\n",
        "for i, example in enumerate(streaming_dataset[\"train\"]):\n",
        "    print(example[\"text\"][:100])\n",
        "    if i >= 5:  # Afficher seulement les 5 premiers exemples\n",
        "        break"
      ],
      "metadata": {
        "id": "eL5YmVxNFpoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercices pratiques**\n",
        "\n",
        "1. Exploration de dataset : Chargez le dataset SQUAD et identifiez les 10 mots les plus fréquents dans les questions."
      ],
      "metadata": {
        "id": "XdAJCw5bFul6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YB36vDYSF8_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Transformation et filtrage : Créez un sous-ensemble du dataset IMDB contenant uniquement les critiques positives de plus de 200 mots."
      ],
      "metadata": {
        "id": "-NHcIwOUGMFe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5AqhgJg0GLs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Création d'un dataset"
      ],
      "metadata": {
        "id": "Gxxp_x5cGe73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A VOUS: Préparez deux versions de votre propre dataset pour un RAG à partir du dataset Wikipedia.dumps (https://huggingface.co/datasets/legacy-datasets/wikipedia):\n",
        "# 1. Une version monolingue (anglais)\n",
        "# 2. Une version trilingue (langues à votre choix)\n",
        "\n",
        "# NB: ne prenez pas le corpus entier, il est trop grand.\n",
        "# Limitez le volume du corpus et choisissez les textes qui appartiennent à un seul domaine (science, physique, linguistique etc.)\n",
        "# Pour le dataset multilingue, rajoutez le label de langue"
      ],
      "metadata": {
        "id": "p2IqschGhjFF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}